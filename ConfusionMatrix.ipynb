{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import DialogueActClassifierFactory\n",
    "from joblib import load\n",
    "from metrics import ConfusionMatrixGenerator\n",
    "from pandas import DataFrame, option_context, read_csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pcc_clf = load('./models/program_comprehension_challenge_classifier.pickle') \n",
    "dac_factory = DialogueActClassifierFactory()\n",
    "dac_clf = dac_factory.get_classifier(classifier_file=Path('./models/dialogue_act_classifier.pickle'), test_set_percentage=10)\n",
    "\n",
    "training_dataset = read_csv('../master-of-engineering/Assets/BigQuery/training_dataset.csv')\n",
    "test_dataset = read_csv('../master-of-engineering/Assets/BigQuery/test_dataset.csv')\n",
    "\n",
    "FEATURES = ['body', 'dialogue_act_classification_ml', 'comment_is_by_author']\n",
    "LABEL = 'program_comprehension_challenge'\n",
    "\n",
    "X_train = training_dataset[FEATURES]\n",
    "X_test = test_dataset[FEATURES]\n",
    "y_train = training_dataset[LABEL]\n",
    "y_true = test_dataset[LABEL]"
   ]
  },
  {
   "source": [
    "# Dialogue Act Classifier Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls = dac_factory.get_precision_and_recall()\n",
    "precisions_recalls = [precisions, recalls]\n",
    "\n",
    "dac_report = {}\n",
    "dac_report[''] = [None, None, dac_factory.get_accuracy()]\n",
    "for label in precisions.keys():\n",
    "  dac_report[label] = [i[label] for i in precisions_recalls]\n",
    "\n",
    "df = DataFrame.from_dict(dac_report, orient='index', columns=['Precision', 'Recall', 'Accuracy'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_cm = dac_factory.get_confusion_matrix()\n",
    "ConfusionMatrixGenerator.print_confusion_matrix(dac_cm._confusion, dac_cm._values, number_formatting='.1f', font_size=8)"
   ]
  },
  {
   "source": [
    "# Program Comprehension Challenge Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_clf.fit(X_train, y_train)\n",
    "y_pred = pcc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "report = metrics.classification_report(y_true, y_pred, digits=8)\n",
    "print(report)\n",
    "cm = metrics.confusion_matrix(y_true, y_pred, labels=[\"Yes\", \"No\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixGenerator.print_confusion_matrix(cm, [\"Yes\", \"No\"], figsize=[4, 4], number_formatting='.2f', font_size=12)"
   ]
  }
 ]
}